{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a4866d8",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22a56046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import zeros, arange, subplots, plt, savefig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e22ff4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, root_dir, split, embedding_dimensionality):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.embedding_dimensionality = embedding_dimensionality\n",
    "        self.preprocess = transforms.Compose([transforms.RandomHorizontalFlip(), \n",
    "                                              transforms.RandomCrop(224), \n",
    "                                              transforms.ToTensor(), \n",
    "                                              transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                                   std=[0.229, 0.224, 0.225])\n",
    "                                             ])\n",
    "\n",
    "        print(f'Loading data from {split}')\n",
    "        gt_file = root_dir + split\n",
    "        \n",
    "        # count number of images in the split\n",
    "        num_lines = 0\n",
    "        with open(gt_file, 'r') as f:\n",
    "            for i, l in enumerate(f):\n",
    "                pass\n",
    "        num_lines = i + 1\n",
    "\n",
    "        # Load img IDs and caption embeddings to memory\n",
    "        print(\"Num lines: \" + str(num_lines))\n",
    "        self.img_ids = np.empty([num_lines], dtype=\"S50\")\n",
    "        self.captions_embeddings = np.zeros((num_lines, self.embedding_dimensionality), dtype=np.float32)\n",
    "        print(\"Loading labels ...\")\n",
    "        with open(gt_file, 'r') as annsfile:\n",
    "            for c, i in enumerate(annsfile):\n",
    "                if c == num_lines: \n",
    "                    break\n",
    "                id_, vec = i.split('\\t')\n",
    "                vec = vec.strip().split(',')\n",
    "                self.img_ids[c] = id_\n",
    "                # Load caption word2vec embedding\n",
    "                for l in range(0, self.embedding_dimensionality):\n",
    "                    self.captions_embeddings[c, l] = float(vec[l])\n",
    "\n",
    "        print(\"Data read.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_ids[idx].decode('utf-8')\n",
    "        input_img = Image.open(self.root_dir + 'images/newyork/' + img_name + '.jpg').convert('RGB')\n",
    "        img_tensor = self.preprocess(input_img)\n",
    "        target_tensor = torch.from_numpy(self.captions_embeddings[idx, :])\n",
    "        return img_name, img_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf2ebd6",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fc4b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, embedding_dimensionality):\n",
    "        super(Model, self).__init__()\n",
    "        self.cnn = models.resnet50(pretrained=True, num_classes=embedding_dimensionality)\n",
    "\n",
    "    def forward(self, image):\n",
    "        x = self.cnn(image)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1ff673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2624980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, print_freq, plot_data, gpu):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    loss_meter = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (img_name, image, target) in enumerate(train_loader):\n",
    "        target_var = torch.autograd.Variable(target).cuda(gpu)\n",
    "        image_var = torch.autograd.Variable(image)\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # compute output\n",
    "        output = model(image_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure and record loss\n",
    "        loss_meter.update(loss.data.item(), image.size()[0])\n",
    "      \n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.8f} ({loss.avg:.8f})\\t'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=loss_meter))\n",
    "\n",
    "    plot_data['train_loss'][plot_data['epoch']] = loss_meter.avg\n",
    "\n",
    "    return plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8301387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, print_freq, plot_data, gpu):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        batch_time = AverageMeter()\n",
    "        loss_meter = AverageMeter()\n",
    "\n",
    "        # switch to evaluate mode\n",
    "        model.eval()\n",
    "\n",
    "        end = time.time()\n",
    "        for i, (img_name, image, target) in enumerate(val_loader):\n",
    "            target_var = torch.autograd.Variable(target).cuda(gpu)\n",
    "            image_var = torch.autograd.Variable(image)\n",
    "\n",
    "            # compute output\n",
    "            output = model(image_var)\n",
    "            loss = criterion(output, target_var)\n",
    "\n",
    "            # measure and record loss\n",
    "            loss_meter.update(loss.data.item(), image.size()[0])\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                          'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.8f} ({loss.avg:.8f})\\t'.format(\n",
    "                       i, len(val_loader), batch_time=batch_time, loss=loss_meter))\n",
    "\n",
    "        plot_data['val_loss'][plot_data['epoch']] = loss_meter.avg\n",
    "\n",
    "    return plot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce40cf0d",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52f51afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_id = 'multi-modal'\n",
    "dataset_root = './data/'\n",
    "split_train = 'embedding/caption_embedding.csv'\n",
    "split_val = 'embedding/caption_embedding.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "078cfed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimensionality = 1000  # Number of CNN outputs (dimensionality of the word2vec model)\n",
    "batch_size = 64 # Set as large as possible\n",
    "epochs = 10 # Converges around y\n",
    "print_freq = 1 # How frequently print loss in screen\n",
    "plot = True  # Save a plot with the training and validation losses\n",
    "workers = 64 # Num of data loading workers\n",
    "gpu = 0\n",
    "lr = 0.01 # 0.01 Is a good start\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# Set model and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss().cuda(gpu) # Sigmoid + Cross Entropy Loss \n",
    "\n",
    "# Create ResNet50 model with custom number of outputs\n",
    "model = Model(embedding_dimensionality=embedding_dimensionality).cuda(gpu)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr) # ADAM optimizer\n",
    "model = torch.nn.DataParallel(model, device_ids=[gpu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c1286c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from embedding/caption_embedding.csv\n",
      "Num lines: 10\n",
      "Loading labels ...\n",
      "Data read.\n",
      "Loading data from embedding/caption_embedding.csv\n",
      "Num lines: 10\n",
      "Loading labels ...\n",
      "Data read.\n",
      "Dataset and model ready. Starting training ...\n",
      "Epoch: [0][0/1]\tTime 6.729 (6.729)\tData 1.168 (1.168)\tLoss 1.06297028 (1.06297028)\t\n",
      "Test: [0/1]\tTime 1.088 (1.088)\tLoss 359588.93750000 (359588.93750000)\t\n",
      "{'train_loss': array([1.06297028, 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ]), 'val_loss': array([359588.9375,      0.    ,      0.    ,      0.    ,      0.    ,\n",
      "            0.    ,      0.    ,      0.    ,      0.    ,      0.    ]), 'epoch': 0}\n",
      "Epoch: [1][0/1]\tTime 1.289 (1.289)\tData 1.017 (1.017)\tLoss 1.46171165 (1.46171165)\t\n",
      "Test: [0/1]\tTime 1.131 (1.131)\tLoss 3571882917888.00000000 (3571882917888.00000000)\t\n",
      "{'train_loss': array([1.06297028, 1.46171165, 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ]), 'val_loss': array([3.59588938e+05, 3.57188292e+12, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00]), 'epoch': 1}\n",
      "Epoch: [2][0/1]\tTime 1.224 (1.224)\tData 0.982 (0.982)\tLoss 0.91550487 (0.91550487)\t\n",
      "Test: [0/1]\tTime 1.125 (1.125)\tLoss 4201075821248512.00000000 (4201075821248512.00000000)\t\n",
      "{'train_loss': array([1.06297028, 1.46171165, 0.91550487, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ]), 'val_loss': array([3.59588938e+05, 3.57188292e+12, 4.20107582e+15, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00]), 'epoch': 2}\n",
      "Epoch: [3][0/1]\tTime 1.269 (1.269)\tData 1.034 (1.034)\tLoss 0.84184939 (0.84184939)\t\n",
      "Test: [0/1]\tTime 1.134 (1.134)\tLoss 3658306551808.00000000 (3658306551808.00000000)\t\n",
      "{'train_loss': array([1.06297028, 1.46171165, 0.91550487, 0.84184939, 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ]), 'val_loss': array([3.59588938e+05, 3.57188292e+12, 4.20107582e+15, 3.65830655e+12,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00]), 'epoch': 3}\n",
      "Epoch: [4][0/1]\tTime 1.385 (1.385)\tData 1.150 (1.150)\tLoss 0.76820564 (0.76820564)\t\n",
      "Test: [0/1]\tTime 1.123 (1.123)\tLoss 134914367488.00000000 (134914367488.00000000)\t\n",
      "{'train_loss': array([1.06297028, 1.46171165, 0.91550487, 0.84184939, 0.76820564,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ]), 'val_loss': array([3.59588938e+05, 3.57188292e+12, 4.20107582e+15, 3.65830655e+12,\n",
      "       1.34914367e+11, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00]), 'epoch': 4}\n",
      "Epoch: [5][0/1]\tTime 1.312 (1.312)\tData 1.077 (1.077)\tLoss 0.79881930 (0.79881930)\t\n",
      "Test: [0/1]\tTime 1.136 (1.136)\tLoss 36002435072.00000000 (36002435072.00000000)\t\n",
      "{'train_loss': array([1.06297028, 1.46171165, 0.91550487, 0.84184939, 0.76820564,\n",
      "       0.7988193 , 0.        , 0.        , 0.        , 0.        ]), 'val_loss': array([3.59588938e+05, 3.57188292e+12, 4.20107582e+15, 3.65830655e+12,\n",
      "       1.34914367e+11, 3.60024351e+10, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00]), 'epoch': 5}\n",
      "Epoch: [6][0/1]\tTime 1.247 (1.247)\tData 1.013 (1.013)\tLoss 0.75380778 (0.75380778)\t\n",
      "Test: [0/1]\tTime 1.119 (1.119)\tLoss 4362176512.00000000 (4362176512.00000000)\t\n",
      "{'train_loss': array([1.06297028, 1.46171165, 0.91550487, 0.84184939, 0.76820564,\n",
      "       0.7988193 , 0.75380778, 0.        , 0.        , 0.        ]), 'val_loss': array([3.59588938e+05, 3.57188292e+12, 4.20107582e+15, 3.65830655e+12,\n",
      "       1.34914367e+11, 3.60024351e+10, 4.36217651e+09, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00]), 'epoch': 6}\n",
      "Epoch: [7][0/1]\tTime 1.266 (1.266)\tData 1.032 (1.032)\tLoss 0.72270030 (0.72270030)\t\n",
      "Test: [0/1]\tTime 1.118 (1.118)\tLoss 960454592.00000000 (960454592.00000000)\t\n",
      "{'train_loss': array([1.06297028, 1.46171165, 0.91550487, 0.84184939, 0.76820564,\n",
      "       0.7988193 , 0.75380778, 0.7227003 , 0.        , 0.        ]), 'val_loss': array([3.59588938e+05, 3.57188292e+12, 4.20107582e+15, 3.65830655e+12,\n",
      "       1.34914367e+11, 3.60024351e+10, 4.36217651e+09, 9.60454592e+08,\n",
      "       0.00000000e+00, 0.00000000e+00]), 'epoch': 7}\n",
      "Epoch: [8][0/1]\tTime 1.260 (1.260)\tData 1.025 (1.025)\tLoss 0.72241658 (0.72241658)\t\n",
      "Test: [0/1]\tTime 1.142 (1.142)\tLoss 256690560.00000000 (256690560.00000000)\t\n",
      "{'train_loss': array([1.06297028, 1.46171165, 0.91550487, 0.84184939, 0.76820564,\n",
      "       0.7988193 , 0.75380778, 0.7227003 , 0.72241658, 0.        ]), 'val_loss': array([3.59588938e+05, 3.57188292e+12, 4.20107582e+15, 3.65830655e+12,\n",
      "       1.34914367e+11, 3.60024351e+10, 4.36217651e+09, 9.60454592e+08,\n",
      "       2.56690560e+08, 0.00000000e+00]), 'epoch': 8}\n",
      "Epoch: [9][0/1]\tTime 1.286 (1.286)\tData 1.052 (1.052)\tLoss 0.70778042 (0.70778042)\t\n",
      "Test: [0/1]\tTime 1.107 (1.107)\tLoss 102874136.00000000 (102874136.00000000)\t\n",
      "{'train_loss': array([1.06297028, 1.46171165, 0.91550487, 0.84184939, 0.76820564,\n",
      "       0.7988193 , 0.75380778, 0.7227003 , 0.72241658, 0.70778042]), 'val_loss': array([3.59588938e+05, 3.57188292e+12, 4.20107582e+15, 3.65830655e+12,\n",
      "       1.34914367e+11, 3.60024351e+10, 4.36217651e+09, 9.60454592e+08,\n",
      "       2.56690560e+08, 1.02874136e+08]), 'epoch': 9}\n",
      "Training completed for 10 epochs.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "\n",
    "# Data loading code (pin_memory allows better transferring of samples to GPU memory)\n",
    "train_dataset = Dataset(dataset_root, split_train, embedding_dimensionality)\n",
    "val_dataset = Dataset(dataset_root, split_val, embedding_dimensionality)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True, \n",
    "                                           num_workers=workers, \n",
    "                                           pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, \n",
    "                                         batch_size=batch_size, \n",
    "                                         shuffle=True, \n",
    "                                         num_workers=workers, \n",
    "                                         pin_memory=True)\n",
    "\n",
    "\n",
    "# Plotting is not needed if we don't want to monitor training\n",
    "# Also, standard monitoring tools such as Visom or Tensorflow could be used.\n",
    "# Plotting config\n",
    "plot_data = {}\n",
    "plot_data['train_loss'] = zeros(epochs)\n",
    "plot_data['val_loss'] = zeros(epochs)\n",
    "plot_data['epoch'] = 0\n",
    "it_axes = arange(epochs)\n",
    "_, ax1 = subplots()\n",
    "ax2 = ax1.twinx()\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.set_ylabel('train loss (r), val loss (y)')\n",
    "ax1.set_ylim([0, 2])\n",
    "best_loss = 1000\n",
    "\n",
    "\n",
    "print(\"Dataset and model ready. Starting training ...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    plot_data['epoch'] = epoch\n",
    "    # Train for one epoch\n",
    "    plot_data = train(train_loader, model, criterion, optimizer, epoch, print_freq, plot_data, gpu)\n",
    "    # Evaluate on validation set\n",
    "    plot_data = validate(val_loader, model, criterion, print_freq, plot_data, gpu)\n",
    "    \n",
    "    print(plot_data)\n",
    "\n",
    "\n",
    "    if plot:\n",
    "\n",
    "        ax1.plot(it_axes[0:epoch+1], plot_data['train_loss'][0:epoch+1], 'r')\n",
    "        ax1.plot(it_axes[0:epoch+1], plot_data['val_loss'][0:epoch+1], 'y')\n",
    "        plt.grid(True)\n",
    "        plt.title(training_id)\n",
    "\n",
    "        # Save graph to disk\n",
    "        if epoch % 1 == 0 and epoch != 0:\n",
    "            title = dataset_root +'training/' + training_id + '_epoch_' + str(epoch) + '.png'\n",
    "            savefig(title, bbox_inches='tight')\n",
    "\n",
    "print(\"Training completed for \" + str(epochs) + \" epochs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d3767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e22718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac37d133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c41e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad78097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf15c50d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49db60bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f7593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65ad0a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dfc5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
