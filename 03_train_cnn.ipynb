{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c544d68",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c1eb5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4c4978",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, root_dir, split, embedding_dimensionality):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.embedding_dimensionality = embedding_dimensionality\n",
    "        self.preprocess = transforms.Compose([transforms.RandomHorizontalFlip(), \n",
    "                                              transforms.RandomCrop(224), \n",
    "                                              transforms.ToTensor(), \n",
    "                                              transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                                   std=[0.229, 0.224, 0.225])\n",
    "                                             ])\n",
    "\n",
    "        print(f'Loading data from {split}')\n",
    "        gt_file = root_dir + split\n",
    "        \n",
    "        # count number of images in the split\n",
    "        num_lines = 0\n",
    "        with open(gt_file, 'r') as f:\n",
    "            for i, l in enumerate(f):\n",
    "                pass\n",
    "        num_lines = i + 1\n",
    "\n",
    "        # Load img IDs and caption embeddings to memory\n",
    "        print(\"Num lines: \" + str(num_lines))\n",
    "        self.img_ids = np.empty([num_lines], dtype=\"S50\")\n",
    "        self.captions_embeddings = np.zeros((num_lines, self.embedding_dimensionality), dtype=np.float32)\n",
    "        print(\"Loading labels ...\")\n",
    "        with open(gt_file, 'r') as annsfile:\n",
    "            for c, i in enumerate(annsfile):\n",
    "                if c == num_lines: \n",
    "                    break\n",
    "                id_, vec = i.split('\\t')\n",
    "                vec = vec.strip().split(',')\n",
    "                self.img_ids[c] = id_\n",
    "                # Load caption word2vec embedding\n",
    "                for l in range(0, self.embedding_dimensionality):\n",
    "                    self.captions_embeddings[c, l] = float(vec[l])\n",
    "\n",
    "        print(\"Data read.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_ids[idx].decode('utf-8')\n",
    "        input_img = Image.open(self.root_dir + 'images/newyork/' + img_name + '.jpg').convert('RGB')\n",
    "        img_tensor = self.preprocess(input_img)\n",
    "        target_tensor = torch.from_numpy(self.captions_embeddings[idx, :])\n",
    "        return img_name, img_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da6a33a",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a291ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, embedding_dimensionality):\n",
    "        super(Model, self).__init__()\n",
    "        self.cnn = models.resnet50(pretrained=True, num_classes=embedding_dimensionality)\n",
    "\n",
    "    def forward(self, image):\n",
    "        x = self.cnn(image)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1086777",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import glob\n",
    "import os\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f490c0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7d6199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, print_freq, plot_data, gpu):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    loss_meter = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (img_name, image, target) in enumerate(train_loader):\n",
    "        target_var = torch.autograd.Variable(target).cuda(gpu)\n",
    "        image_var = torch.autograd.Variable(image)\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # compute output\n",
    "        output = model(image_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure and record loss\n",
    "        loss_meter.update(loss.data.item(), image.size()[0])\n",
    "      \n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.8f} ({loss.avg:.8f})\\t'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=loss_meter))\n",
    "\n",
    "    plot_data['train_loss'][plot_data['epoch']] = loss_meter.avg\n",
    "\n",
    "    return plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "405bda1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, print_freq, plot_data, gpu):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        batch_time = AverageMeter()\n",
    "        loss_meter = AverageMeter()\n",
    "\n",
    "        # switch to evaluate mode\n",
    "        model.eval()\n",
    "\n",
    "        end = time.time()\n",
    "        for i, (img_name, image, target) in enumerate(val_loader):\n",
    "            target_var = torch.autograd.Variable(target).cuda(gpu)\n",
    "            image_var = torch.autograd.Variable(image)\n",
    "\n",
    "            # compute output\n",
    "            output = model(image_var)\n",
    "            loss = criterion(output, target_var)\n",
    "\n",
    "            # measure and record loss\n",
    "            loss_meter.update(loss.data.item(), image.size()[0])\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                          'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.8f} ({loss.avg:.8f})\\t'.format(\n",
    "                       i, len(val_loader), batch_time=batch_time, loss=loss_meter))\n",
    "\n",
    "        plot_data['val_loss'][plot_data['epoch']] = loss_meter.avg\n",
    "\n",
    "    return plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079aebfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca3a760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e794789c",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36fd1c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import zeros, arange, subplots, plt, savefig\n",
    "from torch.hub import load_state_dict_from_url\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "305fbcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_id = 'multi-modal'\n",
    "dataset_root = './data/'\n",
    "split_train = 'embedding/caption_embedding.csv'\n",
    "split_val = 'embedding/caption_embedding.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c014beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimensionality = 1000  # Number of CNN outputs (dimensionality of the word2vec model)\n",
    "batch_size = 64 # Set as large as possible\n",
    "epochs = 10 # Converges around y\n",
    "print_freq = 1 # How frequently print loss in screen\n",
    "plot = True  # Save a plot with the training and validation losses\n",
    "workers = 64 # Num of data loading workers\n",
    "gpu = 0\n",
    "lr = 0.01 # 0.01 Is a good start\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# Set model and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss().cuda(gpu) # Sigmoid + Cross Entropy Loss \n",
    "\n",
    "# Create ResNet50 model with custom number of outputs\n",
    "model = Model(embedding_dimensionality=embedding_dimensionality).cuda(gpu)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr) # ADAM optimizer\n",
    "model = torch.nn.DataParallel(model, device_ids=[gpu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8966c637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from embedding/caption_embedding.csv\n",
      "Num lines: 10\n",
      "Loading labels ...\n",
      "Data read.\n",
      "Loading data from embedding/caption_embedding.csv\n",
      "Num lines: 10\n",
      "Loading labels ...\n",
      "Data read.\n",
      "Dataset and model ready. Starting training ...\n",
      "Epoch: [0][0/1]\tTime 6.596 (6.596)\tData 0.965 (0.965)\tLoss 1.06480885 (1.06480885)\t\n",
      "Test: [0/1]\tTime 1.052 (1.052)\tLoss 459228.87500000 (459228.87500000)\t\n",
      "{'train_loss': array([1.06480885, 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ]), 'val_loss': array([459228.875,      0.   ,      0.   ,      0.   ,      0.   ,\n",
      "            0.   ,      0.   ,      0.   ,      0.   ,      0.   ]), 'epoch': 0}\n",
      "Epoch: [1][0/1]\tTime 1.216 (1.216)\tData 0.982 (0.982)\tLoss 1.65740263 (1.65740263)\t\n",
      "Test: [0/1]\tTime 1.039 (1.039)\tLoss 3511266574336.00000000 (3511266574336.00000000)\t\n",
      "{'train_loss': array([1.06480885, 1.65740263, 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ]), 'val_loss': array([4.59228875e+05, 3.51126657e+12, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00]), 'epoch': 1}\n",
      "Epoch: [2][0/1]\tTime 1.185 (1.185)\tData 0.949 (0.949)\tLoss 0.94390136 (0.94390136)\t\n",
      "Test: [0/1]\tTime 1.087 (1.087)\tLoss 122236157952.00000000 (122236157952.00000000)\t\n",
      "{'train_loss': array([1.06480885, 1.65740263, 0.94390136, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ]), 'val_loss': array([4.59228875e+05, 3.51126657e+12, 1.22236158e+11, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00]), 'epoch': 2}\n",
      "Epoch: [3][0/1]\tTime 1.331 (1.331)\tData 1.097 (1.097)\tLoss 0.85763961 (0.85763961)\t\n",
      "Test: [0/1]\tTime 1.001 (1.001)\tLoss 1781593145344.00000000 (1781593145344.00000000)\t\n",
      "{'train_loss': array([1.06480885, 1.65740263, 0.94390136, 0.85763961, 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ]), 'val_loss': array([4.59228875e+05, 3.51126657e+12, 1.22236158e+11, 1.78159315e+12,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00]), 'epoch': 3}\n",
      "Epoch: [4][0/1]\tTime 1.180 (1.180)\tData 0.946 (0.946)\tLoss 0.77328604 (0.77328604)\t\n",
      "Test: [0/1]\tTime 1.102 (1.102)\tLoss 1904250585088.00000000 (1904250585088.00000000)\t\n",
      "{'train_loss': array([1.06480885, 1.65740263, 0.94390136, 0.85763961, 0.77328604,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ]), 'val_loss': array([4.59228875e+05, 3.51126657e+12, 1.22236158e+11, 1.78159315e+12,\n",
      "       1.90425059e+12, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00]), 'epoch': 4}\n",
      "Epoch: [5][0/1]\tTime 1.196 (1.196)\tData 0.962 (0.962)\tLoss 0.73249662 (0.73249662)\t\n",
      "Test: [0/1]\tTime 1.016 (1.016)\tLoss 62139113472.00000000 (62139113472.00000000)\t\n",
      "{'train_loss': array([1.06480885, 1.65740263, 0.94390136, 0.85763961, 0.77328604,\n",
      "       0.73249662, 0.        , 0.        , 0.        , 0.        ]), 'val_loss': array([4.59228875e+05, 3.51126657e+12, 1.22236158e+11, 1.78159315e+12,\n",
      "       1.90425059e+12, 6.21391135e+10, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00]), 'epoch': 5}\n",
      "Epoch: [6][0/1]\tTime 1.207 (1.207)\tData 0.973 (0.973)\tLoss 0.72329140 (0.72329140)\t\n",
      "Test: [0/1]\tTime 1.088 (1.088)\tLoss 2101262848.00000000 (2101262848.00000000)\t\n",
      "{'train_loss': array([1.06480885, 1.65740263, 0.94390136, 0.85763961, 0.77328604,\n",
      "       0.73249662, 0.7232914 , 0.        , 0.        , 0.        ]), 'val_loss': array([4.59228875e+05, 3.51126657e+12, 1.22236158e+11, 1.78159315e+12,\n",
      "       1.90425059e+12, 6.21391135e+10, 2.10126285e+09, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00]), 'epoch': 6}\n",
      "Epoch: [7][0/1]\tTime 1.176 (1.176)\tData 0.942 (0.942)\tLoss 0.72358221 (0.72358221)\t\n",
      "Test: [0/1]\tTime 1.071 (1.071)\tLoss 111541272.00000000 (111541272.00000000)\t\n",
      "{'train_loss': array([1.06480885, 1.65740263, 0.94390136, 0.85763961, 0.77328604,\n",
      "       0.73249662, 0.7232914 , 0.72358221, 0.        , 0.        ]), 'val_loss': array([4.59228875e+05, 3.51126657e+12, 1.22236158e+11, 1.78159315e+12,\n",
      "       1.90425059e+12, 6.21391135e+10, 2.10126285e+09, 1.11541272e+08,\n",
      "       0.00000000e+00, 0.00000000e+00]), 'epoch': 7}\n",
      "Epoch: [8][0/1]\tTime 1.163 (1.163)\tData 0.929 (0.929)\tLoss 0.72847837 (0.72847837)\t\n",
      "Test: [0/1]\tTime 1.050 (1.050)\tLoss 7954235.00000000 (7954235.00000000)\t\n",
      "{'train_loss': array([1.06480885, 1.65740263, 0.94390136, 0.85763961, 0.77328604,\n",
      "       0.73249662, 0.7232914 , 0.72358221, 0.72847837, 0.        ]), 'val_loss': array([4.59228875e+05, 3.51126657e+12, 1.22236158e+11, 1.78159315e+12,\n",
      "       1.90425059e+12, 6.21391135e+10, 2.10126285e+09, 1.11541272e+08,\n",
      "       7.95423500e+06, 0.00000000e+00]), 'epoch': 8}\n",
      "Epoch: [9][0/1]\tTime 1.190 (1.190)\tData 0.956 (0.956)\tLoss 0.70347351 (0.70347351)\t\n",
      "Test: [0/1]\tTime 1.195 (1.195)\tLoss 929432.25000000 (929432.25000000)\t\n",
      "{'train_loss': array([1.06480885, 1.65740263, 0.94390136, 0.85763961, 0.77328604,\n",
      "       0.73249662, 0.7232914 , 0.72358221, 0.72847837, 0.70347351]), 'val_loss': array([4.59228875e+05, 3.51126657e+12, 1.22236158e+11, 1.78159315e+12,\n",
      "       1.90425059e+12, 6.21391135e+10, 2.10126285e+09, 1.11541272e+08,\n",
      "       7.95423500e+06, 9.29432250e+05]), 'epoch': 9}\n",
      "Training completed for 10 epochs.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "\n",
    "# Data loading code (pin_memory allows better transferring of samples to GPU memory)\n",
    "train_dataset = Dataset(dataset_root, split_train, embedding_dimensionality)\n",
    "val_dataset = Dataset(dataset_root, split_val, embedding_dimensionality)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True, \n",
    "                                           num_workers=workers, \n",
    "                                           pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, \n",
    "                                         batch_size=batch_size, \n",
    "                                         shuffle=True, \n",
    "                                         num_workers=workers, \n",
    "                                         pin_memory=True)\n",
    "\n",
    "\n",
    "# Plotting is not needed if we don't want to monitor training\n",
    "# Also, standard monitoring tools such as Visom or Tensorflow could be used.\n",
    "# Plotting config\n",
    "plot_data = {}\n",
    "plot_data['train_loss'] = zeros(epochs)\n",
    "plot_data['val_loss'] = zeros(epochs)\n",
    "plot_data['epoch'] = 0\n",
    "it_axes = arange(epochs)\n",
    "_, ax1 = subplots()\n",
    "ax2 = ax1.twinx()\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.set_ylabel('train loss (r), val loss (y)')\n",
    "ax1.set_ylim([0, 2])\n",
    "best_loss = 1000\n",
    "\n",
    "\n",
    "print(\"Dataset and model ready. Starting training ...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    plot_data['epoch'] = epoch\n",
    "    # Train for one epoch\n",
    "    plot_data = train(train_loader, model, criterion, optimizer, epoch, print_freq, plot_data, gpu)\n",
    "    # Evaluate on validation set\n",
    "    plot_data = validate(val_loader, model, criterion, print_freq, plot_data, gpu)\n",
    "    \n",
    "    print(plot_data)\n",
    "\n",
    "\n",
    "    if plot:\n",
    "\n",
    "        ax1.plot(it_axes[0:epoch+1], plot_data['train_loss'][0:epoch+1], 'r')\n",
    "        ax1.plot(it_axes[0:epoch+1], plot_data['val_loss'][0:epoch+1], 'y')\n",
    "        plt.grid(True)\n",
    "        plt.title(training_id)\n",
    "\n",
    "        # Save graph to disk\n",
    "        if epoch % 1 == 0 and epoch != 0:\n",
    "            title = dataset_root +'training/' + training_id + '_epoch_' + str(epoch) + '.png'\n",
    "            savefig(title, bbox_inches='tight')\n",
    "\n",
    "print(\"Training completed for \" + str(epochs) + \" epochs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73585d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e39e7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d133223d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aef4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c22eb0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8733de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deace855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec185074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f707c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a8265d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
