{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f5c9272",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7e6352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import zeros, arange, subplots, plt, savefig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4634a8",
   "metadata": {},
   "source": [
    "#### Setup logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b630a039",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a04b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using torch: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Using torch: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa154a78",
   "metadata": {},
   "source": [
    "#### Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e5427e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = './data/'\n",
    "CAPTIONS_TRAIN_SET_PATH = 'embedding/caption_embedding.csv'\n",
    "CAPTIONS_VALIDATION_SET_PATH = 'embedding/caption_embedding.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97b0f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSIONALITY = 1000 # number of CNN outputs = dimensionality of the captions word2vec model\n",
    "BATCH_SIZE = 4 \n",
    "EPOCHS = 10\n",
    "WORKERS = 4 # number of data loading workers = CPU cores\n",
    "GPU = 0\n",
    "LR = 0.01 \n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e1d51cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e996f61",
   "metadata": {},
   "source": [
    "#### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "280f3e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, embedding_dimensionality):\n",
    "        super(Model, self).__init__()\n",
    "        self.cnn = models.resnet50(pretrained=True, num_classes=embedding_dimensionality)\n",
    "\n",
    "    def forward(self, image):\n",
    "        x = self.cnn(image)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d81073",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb7f89d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss().cuda(GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b471af7",
   "metadata": {},
   "source": [
    "Create ResNet50 model with custom number of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d82220e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(embedding_dimensionality=EMBEDDING_DIMENSIONALITY).cuda(GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "703b9c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.01\n",
       "    weight_decay: 0.0001\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6244a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.DataParallel(model, device_ids=[GPU])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d57a82",
   "metadata": {},
   "source": [
    "#### Define and load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecc45567",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, root_dir, split, embedding_dimensionality):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.embedding_dimensionality = embedding_dimensionality\n",
    "        self.preprocess = transforms.Compose([transforms.RandomHorizontalFlip(), \n",
    "                                              transforms.RandomCrop(224), \n",
    "                                              transforms.ToTensor(), \n",
    "                                              transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                                   std=[0.229, 0.224, 0.225])])\n",
    "        logger.info(f'Loading data from {split}')\n",
    "        \n",
    "        # count number of images in the split\n",
    "        n = 0\n",
    "        with open(f'{DATASET_ROOT}{CAPTIONS_TRAIN_SET_PATH}', 'r') as f:\n",
    "            for _, _ in enumerate(f):\n",
    "                n += 1\n",
    "                \n",
    "        # placeholder for image ids - dummy bytes\n",
    "        self.img_ids = np.empty([n], dtype='S50')\n",
    "        # placeholder for captions embedding - [number of captions * vector dimension]\n",
    "        self.captions_embeddings = np.zeros((n, self.embedding_dimensionality), dtype=np.float32)\n",
    "        \n",
    "        # populate the placeholders \n",
    "        with open(f'{DATASET_ROOT}{CAPTIONS_TRAIN_SET_PATH}', 'r') as f:\n",
    "            for idx, row in enumerate(f):\n",
    "                uid, vec = row.split('\\t')\n",
    "                vec = vec.strip().split(',')\n",
    "                self.img_ids[idx] = uid\n",
    "                for i in range(self.embedding_dimensionality):\n",
    "                    self.captions_embeddings[idx, i] = float(vec[i])\n",
    "        logger.info(f'Caption embedding shape = {self.captions_embeddings[0].shape}')\n",
    "        logger.info('Data loading done.')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx].decode('utf-8')\n",
    "        img = Image.open(f'{DATASET_ROOT}images/{img_id}.jpg').convert('RGB')\n",
    "        img_tensor = self.preprocess(img)\n",
    "        target_tensor = torch.from_numpy(self.captions_embeddings[idx, :])\n",
    "        return img_id, img_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cebac6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data from embedding/caption_embedding.csv\n",
      "Caption embedding shape = (1000,)\n",
      "Data loading done.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(DATASET_ROOT, CAPTIONS_TRAIN_SET_PATH, EMBEDDING_DIMENSIONALITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ccdfc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cdc8b7",
   "metadata": {},
   "source": [
    "Dataset object wraps - image id, image tensor and the caption embedding tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b0478f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id, image_tensor, caption_embedding_tensor = train_dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b80ea8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1489658491986857252'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6132d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74ad8ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_embedding_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3b294fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data from embedding/caption_embedding.csv\n",
      "Caption embedding shape = (1000,)\n",
      "Data loading done.\n"
     ]
    }
   ],
   "source": [
    "val_dataset = Dataset(DATASET_ROOT, CAPTIONS_VALIDATION_SET_PATH, EMBEDDING_DIMENSIONALITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f8635",
   "metadata": {},
   "source": [
    "`pin_memory` allows better transferring of samples to GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c982f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           shuffle=True, \n",
    "                                           num_workers=WORKERS, \n",
    "                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96551f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': <__main__.Dataset at 0x7f7bd26880b8>,\n",
       " 'num_workers': 4,\n",
       " 'pin_memory': True,\n",
       " 'timeout': 0,\n",
       " 'worker_init_fn': None,\n",
       " '_DataLoader__multiprocessing_context': None,\n",
       " '_dataset_kind': 0,\n",
       " 'batch_size': 4,\n",
       " 'drop_last': False,\n",
       " 'sampler': <torch.utils.data.sampler.RandomSampler at 0x7f7bd7732b38>,\n",
       " 'batch_sampler': <torch.utils.data.sampler.BatchSampler at 0x7f7bd7732b70>,\n",
       " 'collate_fn': <function torch.utils.data._utils.collate.default_collate(batch)>,\n",
       " '_DataLoader__initialized': True,\n",
       " '_IterableDataset_len_called': None}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcf544bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(val_dataset, \n",
    "                                         batch_size=BATCH_SIZE, \n",
    "                                         shuffle=True, \n",
    "                                         num_workers=WORKERS, \n",
    "                                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138191a3",
   "metadata": {},
   "source": [
    "#### Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1552593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e8179b",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "203b84ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_freq = 1 # how frequently to print loss value to screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "010353e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, print_freq, plot_data, gpu):\n",
    "    batch_time = AverageMeter()\n",
    "    loss_meter = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    end = time.time()\n",
    "    for i, (img_id, image, target) in enumerate(train_loader):\n",
    "        target_var = torch.autograd.Variable(target).cuda(GPU)\n",
    "        image_var = torch.autograd.Variable(image)\n",
    "\n",
    "        # compute output\n",
    "        output = model(image_var)\n",
    "        loss = criterion(output, target_var)\n",
    "        \n",
    "        # measure and record loss\n",
    "        # loss.data.item() => loss value\n",
    "        # image.size()[0] => batch size\n",
    "        loss_meter.update(loss.data.item(), image.size()[0])\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        if i % print_freq == 0:\n",
    "            logger.info(f'Epoch: [{epoch}] | Train Batch: [{i}/{len(train_loader)}]')\n",
    "            logger.info(f'Train Loss: [loss.val={loss_meter.val}] [loss.avg={loss_meter.avg}]')\n",
    "            logger.info(f'Train Batch Time: [{batch_time.avg}]')\n",
    "            \n",
    "    plot_data['train_loss'][plot_data['epoch']] = loss_meter.avg\n",
    "    \n",
    "    return plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0173086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, print_freq, plot_data, gpu):\n",
    "    with torch.no_grad():\n",
    "        batch_time = AverageMeter()\n",
    "        loss_meter = AverageMeter()\n",
    "\n",
    "        # switch to evaluate mode\n",
    "        model.eval()\n",
    "\n",
    "        end = time.time()\n",
    "        for i, (img_id, image, target) in enumerate(val_loader):\n",
    "            target_var = torch.autograd.Variable(target).cuda(GPU)\n",
    "            image_var = torch.autograd.Variable(image)\n",
    "\n",
    "            # compute output\n",
    "            output = model(image_var)\n",
    "            loss = criterion(output, target_var)\n",
    "\n",
    "            # measure and record loss\n",
    "            loss_meter.update(loss.data.item(), image.size()[0])\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            \n",
    "            if i % print_freq == 0:\n",
    "                logger.info(f'Val Batch: [{i}/{len(val_loader)}]')\n",
    "                logger.info(f'Val Loss: [loss.val={loss_meter.val}] [loss.avg={loss_meter.avg}]')\n",
    "                logger.info(f'Val Batch Time: [{batch_time.avg}]')\n",
    "        plot_data['val_loss'][plot_data['epoch']] = loss_meter.avg\n",
    "\n",
    "    return plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b613bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, filename):\n",
    "    logger.info('Saving model checkpoint')\n",
    "    torch.save(model.state_dict(), filename + '.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8d95669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0] | Train Batch: [0/5]\n",
      "Train Loss: [loss.val=0.9345870018005371] [loss.avg=0.9345870018005371]\n",
      "Train Batch Time: [5.205448865890503]\n",
      "Epoch: [0] | Train Batch: [1/5]\n",
      "Train Loss: [loss.val=2.281630039215088] [loss.avg=1.6081085205078125]\n",
      "Train Batch Time: [2.667778730392456]\n",
      "Epoch: [0] | Train Batch: [2/5]\n",
      "Train Loss: [loss.val=1.2691166400909424] [loss.avg=1.4951112270355225]\n",
      "Train Batch Time: [1.825080156326294]\n",
      "Epoch: [0] | Train Batch: [3/5]\n",
      "Train Loss: [loss.val=0.8656741380691528] [loss.avg=1.33775195479393]\n",
      "Train Batch Time: [1.4036982655525208]\n",
      "Epoch: [0] | Train Batch: [4/5]\n",
      "Train Loss: [loss.val=0.8268405199050903] [loss.avg=1.235569667816162]\n",
      "Train Batch Time: [1.1508888244628905]\n",
      "Val Batch: [0/5]\n",
      "Val Loss: [loss.val=6427056275456.0] [loss.avg=6427056275456.0]\n",
      "Val Batch Time: [0.13634896278381348]\n",
      "Val Batch: [1/5]\n",
      "Val Loss: [loss.val=11987515867136.0] [loss.avg=9207286071296.0]\n",
      "Val Batch Time: [0.0874629020690918]\n",
      "Val Batch: [2/5]\n",
      "Val Loss: [loss.val=3799683956736.0] [loss.avg=7404752033109.333]\n",
      "Val Batch Time: [0.07065629959106445]\n",
      "Val Batch: [3/5]\n",
      "Val Loss: [loss.val=6604518850560.0] [loss.avg=7204693737472.0]\n",
      "Val Batch Time: [0.06228315830230713]\n",
      "Val Batch: [4/5]\n",
      "Val Loss: [loss.val=7189882732544.0] [loss.avg=7201731536486.4]\n",
      "Val Batch Time: [0.05741691589355469]\n",
      "Epoch: [1] | Train Batch: [0/5]\n",
      "Train Loss: [loss.val=1.0072834491729736] [loss.avg=1.0072834491729736]\n",
      "Train Batch Time: [0.22886896133422852]\n",
      "Epoch: [1] | Train Batch: [1/5]\n",
      "Train Loss: [loss.val=0.9373775124549866] [loss.avg=0.9723304808139801]\n",
      "Train Batch Time: [0.18388068675994873]\n",
      "Epoch: [1] | Train Batch: [2/5]\n",
      "Train Loss: [loss.val=0.9049144983291626] [loss.avg=0.9498584866523743]\n",
      "Train Batch Time: [0.16912142435709634]\n",
      "Epoch: [1] | Train Batch: [3/5]\n",
      "Train Loss: [loss.val=0.8057921528816223] [loss.avg=0.9138419032096863]\n",
      "Train Batch Time: [0.16174691915512085]\n",
      "Epoch: [1] | Train Batch: [4/5]\n",
      "Train Loss: [loss.val=0.7941381335258484] [loss.avg=0.8899011492729187]\n",
      "Train Batch Time: [0.1572901725769043]\n",
      "Val Batch: [0/5]\n",
      "Val Loss: [loss.val=30483962.0] [loss.avg=30483962.0]\n",
      "Val Batch Time: [0.1348259449005127]\n",
      "Val Batch: [1/5]\n",
      "Val Loss: [loss.val=28987312.0] [loss.avg=29735637.0]\n",
      "Val Batch Time: [0.08684206008911133]\n",
      "Val Batch: [2/5]\n",
      "Val Loss: [loss.val=27642620.0] [loss.avg=29037964.666666668]\n",
      "Val Batch Time: [0.07036495208740234]\n",
      "Val Batch: [3/5]\n",
      "Val Loss: [loss.val=35800132.0] [loss.avg=30728506.5]\n",
      "Val Batch Time: [0.062090277671813965]\n",
      "Val Batch: [4/5]\n",
      "Val Loss: [loss.val=34554400.0] [loss.avg=31493685.2]\n",
      "Val Batch Time: [0.057073450088500975]\n",
      "Epoch: [2] | Train Batch: [0/5]\n",
      "Train Loss: [loss.val=0.7427482008934021] [loss.avg=0.7427482008934021]\n",
      "Train Batch Time: [0.2352430820465088]\n",
      "Epoch: [2] | Train Batch: [1/5]\n",
      "Train Loss: [loss.val=0.7244716286659241] [loss.avg=0.7336099147796631]\n",
      "Train Batch Time: [0.18725132942199707]\n",
      "Epoch: [2] | Train Batch: [2/5]\n",
      "Train Loss: [loss.val=0.7199288010597229] [loss.avg=0.729049543539683]\n",
      "Train Batch Time: [0.17126782735188803]\n",
      "Epoch: [2] | Train Batch: [3/5]\n",
      "Train Loss: [loss.val=0.7383100986480713] [loss.avg=0.7313646823167801]\n",
      "Train Batch Time: [0.1632649302482605]\n",
      "Epoch: [2] | Train Batch: [4/5]\n",
      "Train Loss: [loss.val=0.7354856729507446] [loss.avg=0.732188880443573]\n",
      "Train Batch Time: [0.1586440086364746]\n",
      "Val Batch: [0/5]\n",
      "Val Loss: [loss.val=123332.8515625] [loss.avg=123332.8515625]\n",
      "Val Batch Time: [0.1330714225769043]\n",
      "Val Batch: [1/5]\n",
      "Val Loss: [loss.val=200513.28125] [loss.avg=161923.06640625]\n",
      "Val Batch Time: [0.08599042892456055]\n",
      "Val Batch: [2/5]\n",
      "Val Loss: [loss.val=174194.296875] [loss.avg=166013.4765625]\n",
      "Val Batch Time: [0.06982835133870442]\n",
      "Val Batch: [3/5]\n",
      "Val Loss: [loss.val=91372.0703125] [loss.avg=147353.125]\n",
      "Val Batch Time: [0.061811745166778564]\n",
      "Val Batch: [4/5]\n",
      "Val Loss: [loss.val=114629.3984375] [loss.avg=140808.3796875]\n",
      "Val Batch Time: [0.056894350051879886]\n",
      "Epoch: [3] | Train Batch: [0/5]\n",
      "Train Loss: [loss.val=0.6205911040306091] [loss.avg=0.6205911040306091]\n",
      "Train Batch Time: [0.22549772262573242]\n",
      "Epoch: [3] | Train Batch: [1/5]\n",
      "Train Loss: [loss.val=0.7189908027648926] [loss.avg=0.6697909533977509]\n",
      "Train Batch Time: [0.18214643001556396]\n",
      "Epoch: [3] | Train Batch: [2/5]\n",
      "Train Loss: [loss.val=0.7007737159729004] [loss.avg=0.6801185409228007]\n",
      "Train Batch Time: [0.16782418886820474]\n",
      "Epoch: [3] | Train Batch: [3/5]\n",
      "Train Loss: [loss.val=0.7119050621986389] [loss.avg=0.6880651712417603]\n",
      "Train Batch Time: [0.16078191995620728]\n",
      "Epoch: [3] | Train Batch: [4/5]\n",
      "Train Loss: [loss.val=0.7182154655456543] [loss.avg=0.694095230102539]\n",
      "Train Batch Time: [0.15655031204223632]\n",
      "Val Batch: [0/5]\n",
      "Val Loss: [loss.val=972.4645385742188] [loss.avg=972.4645385742188]\n",
      "Val Batch Time: [0.13399314880371094]\n",
      "Val Batch: [1/5]\n",
      "Val Loss: [loss.val=740.4884033203125] [loss.avg=856.4764709472656]\n",
      "Val Batch Time: [0.08674252033233643]\n",
      "Val Batch: [2/5]\n",
      "Val Loss: [loss.val=430.11328125] [loss.avg=714.3554077148438]\n",
      "Val Batch Time: [0.07019360860188802]\n",
      "Val Batch: [3/5]\n",
      "Val Loss: [loss.val=1724.2523193359375] [loss.avg=966.8296356201172]\n",
      "Val Batch Time: [0.06208223104476929]\n",
      "Val Batch: [4/5]\n",
      "Val Loss: [loss.val=1221.73486328125] [loss.avg=1017.8106811523437]\n",
      "Val Batch Time: [0.057213354110717776]\n",
      "Epoch: [4] | Train Batch: [0/5]\n",
      "Train Loss: [loss.val=0.7041491270065308] [loss.avg=0.7041491270065308]\n",
      "Train Batch Time: [0.228973388671875]\n",
      "Epoch: [4] | Train Batch: [1/5]\n",
      "Train Loss: [loss.val=0.6977529525756836] [loss.avg=0.7009510397911072]\n",
      "Train Batch Time: [0.1843336820602417]\n",
      "Epoch: [4] | Train Batch: [2/5]\n",
      "Train Loss: [loss.val=0.649135947227478] [loss.avg=0.6836793422698975]\n",
      "Train Batch Time: [0.16941610972086588]\n",
      "Epoch: [4] | Train Batch: [3/5]\n",
      "Train Loss: [loss.val=0.6882548928260803] [loss.avg=0.6848232299089432]\n",
      "Train Batch Time: [0.16186171770095825]\n",
      "Epoch: [4] | Train Batch: [4/5]\n",
      "Train Loss: [loss.val=0.692050576210022] [loss.avg=0.6862686991691589]\n",
      "Train Batch Time: [0.15739169120788574]\n",
      "Val Batch: [0/5]\n",
      "Val Loss: [loss.val=24.592622756958008] [loss.avg=24.592622756958008]\n",
      "Val Batch Time: [0.1458432674407959]\n",
      "Val Batch: [1/5]\n",
      "Val Loss: [loss.val=73.00940704345703] [loss.avg=48.80101490020752]\n",
      "Val Batch Time: [0.09256577491760254]\n",
      "Val Batch: [2/5]\n",
      "Val Loss: [loss.val=42.50942611694336] [loss.avg=46.70381863911947]\n",
      "Val Batch Time: [0.07444389661153157]\n",
      "Val Batch: [3/5]\n",
      "Val Loss: [loss.val=19.898727416992188] [loss.avg=40.00254583358765]\n",
      "Val Batch Time: [0.06524783372879028]\n",
      "Val Batch: [4/5]\n",
      "Val Loss: [loss.val=53.299407958984375] [loss.avg=42.661918258666994]\n",
      "Val Batch Time: [0.05967183113098144]\n",
      "New best model by [Val Loss = 42.661918258666994]\n",
      "Saving model checkpoint\n",
      "Epoch: [5] | Train Batch: [0/5]\n",
      "Train Loss: [loss.val=0.6982503533363342] [loss.avg=0.6982503533363342]\n",
      "Train Batch Time: [0.2351086139678955]\n",
      "Epoch: [5] | Train Batch: [1/5]\n",
      "Train Loss: [loss.val=0.653186023235321] [loss.avg=0.6757181882858276]\n",
      "Train Batch Time: [0.1871049404144287]\n",
      "Epoch: [5] | Train Batch: [2/5]\n",
      "Train Loss: [loss.val=0.6928232312202454] [loss.avg=0.6814198692639669]\n",
      "Train Batch Time: [0.17142136891682944]\n",
      "Epoch: [5] | Train Batch: [3/5]\n",
      "Train Loss: [loss.val=0.6843484044075012] [loss.avg=0.6821520030498505]\n",
      "Train Batch Time: [0.16338038444519043]\n",
      "Epoch: [5] | Train Batch: [4/5]\n",
      "Train Loss: [loss.val=0.6826673746109009] [loss.avg=0.6822550773620606]\n",
      "Train Batch Time: [0.15864782333374022]\n",
      "Val Batch: [0/5]\n",
      "Val Loss: [loss.val=2.2041313648223877] [loss.avg=2.2041313648223877]\n",
      "Val Batch Time: [0.1443803310394287]\n",
      "Val Batch: [1/5]\n",
      "Val Loss: [loss.val=8.440632820129395] [loss.avg=5.322382092475891]\n",
      "Val Batch Time: [0.0917428731918335]\n",
      "Val Batch: [2/5]\n",
      "Val Loss: [loss.val=2.808021068572998] [loss.avg=4.484261751174927]\n",
      "Val Batch Time: [0.07358670234680176]\n",
      "Val Batch: [3/5]\n",
      "Val Loss: [loss.val=12.159322738647461] [loss.avg=6.40302699804306]\n",
      "Val Batch Time: [0.0644955039024353]\n",
      "Val Batch: [4/5]\n",
      "Val Loss: [loss.val=10.187572479248047] [loss.avg=7.159936094284058]\n",
      "Val Batch Time: [0.059143447875976564]\n",
      "New best model by [Val Loss = 7.159936094284058]\n",
      "Saving model checkpoint\n",
      "Epoch: [6] | Train Batch: [0/5]\n",
      "Train Loss: [loss.val=0.6876316666603088] [loss.avg=0.6876316666603088]\n",
      "Train Batch Time: [0.2347266674041748]\n",
      "Epoch: [6] | Train Batch: [1/5]\n",
      "Train Loss: [loss.val=0.6907328367233276] [loss.avg=0.6891822516918182]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batch Time: [0.18706262111663818]\n",
      "Epoch: [6] | Train Batch: [2/5]\n",
      "Train Loss: [loss.val=0.6826593279838562] [loss.avg=0.6870079437891642]\n",
      "Train Batch Time: [0.17125733693440756]\n",
      "Epoch: [6] | Train Batch: [3/5]\n",
      "Train Loss: [loss.val=0.6578279137611389] [loss.avg=0.6797129362821579]\n",
      "Train Batch Time: [0.1633630394935608]\n",
      "Epoch: [6] | Train Batch: [4/5]\n",
      "Train Loss: [loss.val=0.6891739964485168] [loss.avg=0.6816051483154297]\n",
      "Train Batch Time: [0.15855112075805664]\n",
      "Val Batch: [0/5]\n",
      "Val Loss: [loss.val=5.606411457061768] [loss.avg=5.606411457061768]\n",
      "Val Batch Time: [0.14147067070007324]\n",
      "Val Batch: [1/5]\n",
      "Val Loss: [loss.val=0.9032280445098877] [loss.avg=3.2548197507858276]\n",
      "Val Batch Time: [0.09042763710021973]\n",
      "Val Batch: [2/5]\n",
      "Val Loss: [loss.val=3.813492774963379] [loss.avg=3.4410440921783447]\n",
      "Val Batch Time: [0.0728610356648763]\n",
      "Val Batch: [3/5]\n",
      "Val Loss: [loss.val=4.088891506195068] [loss.avg=3.6030059456825256]\n",
      "Val Batch Time: [0.06419903039932251]\n",
      "Val Batch: [4/5]\n",
      "Val Loss: [loss.val=0.7588741183280945] [loss.avg=3.0341795802116396]\n",
      "Val Batch Time: [0.05899567604064941]\n",
      "New best model by [Val Loss = 3.0341795802116396]\n",
      "Saving model checkpoint\n",
      "Epoch: [7] | Train Batch: [0/5]\n",
      "Train Loss: [loss.val=0.6806560158729553] [loss.avg=0.6806560158729553]\n",
      "Train Batch Time: [0.23309898376464844]\n",
      "Epoch: [7] | Train Batch: [1/5]\n",
      "Train Loss: [loss.val=0.6895412802696228] [loss.avg=0.6850986480712891]\n",
      "Train Batch Time: [0.18637025356292725]\n",
      "Epoch: [7] | Train Batch: [2/5]\n",
      "Train Loss: [loss.val=0.6889491081237793] [loss.avg=0.6863821347554525]\n",
      "Train Batch Time: [0.17086251576741537]\n",
      "Epoch: [7] | Train Batch: [3/5]\n",
      "Train Loss: [loss.val=0.6809283494949341] [loss.avg=0.6850186884403229]\n",
      "Train Batch Time: [0.16314196586608887]\n",
      "Epoch: [7] | Train Batch: [4/5]\n",
      "Train Loss: [loss.val=0.6940810680389404] [loss.avg=0.6868311643600464]\n",
      "Train Batch Time: [0.15842771530151367]\n",
      "Val Batch: [0/5]\n",
      "Val Loss: [loss.val=2.3252506256103516] [loss.avg=2.3252506256103516]\n",
      "Val Batch Time: [0.1417529582977295]\n",
      "Val Batch: [1/5]\n",
      "Val Loss: [loss.val=1.0725382566452026] [loss.avg=1.698894441127777]\n",
      "Val Batch Time: [0.090476393699646]\n",
      "Val Batch: [2/5]\n",
      "Val Loss: [loss.val=1.6440715789794922] [loss.avg=1.6806201537450154]\n",
      "Val Batch Time: [0.0726629098256429]\n",
      "Val Batch: [3/5]\n",
      "Val Loss: [loss.val=1.4238909482955933] [loss.avg=1.61643785238266]\n",
      "Val Batch Time: [0.0638694167137146]\n",
      "Val Batch: [4/5]\n",
      "Val Loss: [loss.val=3.6790740489959717] [loss.avg=2.0289650917053224]\n",
      "Val Batch Time: [0.05869932174682617]\n",
      "New best model by [Val Loss = 2.0289650917053224]\n",
      "Saving model checkpoint\n",
      "Epoch: [8] | Train Batch: [0/5]\n",
      "Train Loss: [loss.val=0.6850640177726746] [loss.avg=0.6850640177726746]\n",
      "Train Batch Time: [0.23680353164672852]\n",
      "Epoch: [8] | Train Batch: [1/5]\n",
      "Train Loss: [loss.val=0.6896852254867554] [loss.avg=0.687374621629715]\n",
      "Train Batch Time: [0.18797004222869873]\n",
      "Epoch: [8] | Train Batch: [2/5]\n",
      "Train Loss: [loss.val=0.6873153448104858] [loss.avg=0.6873548626899719]\n",
      "Train Batch Time: [0.17192324002583823]\n",
      "Epoch: [8] | Train Batch: [3/5]\n",
      "Train Loss: [loss.val=0.6587932109832764] [loss.avg=0.680214449763298]\n",
      "Train Batch Time: [0.16375911235809326]\n",
      "Epoch: [8] | Train Batch: [4/5]\n",
      "Train Loss: [loss.val=0.6806861758232117] [loss.avg=0.6803087949752807]\n",
      "Train Batch Time: [0.1590947151184082]\n",
      "Val Batch: [0/5]\n",
      "Val Loss: [loss.val=0.5805449485778809] [loss.avg=0.5805449485778809]\n",
      "Val Batch Time: [0.14743971824645996]\n",
      "Val Batch: [1/5]\n",
      "Val Loss: [loss.val=0.6801841259002686] [loss.avg=0.6303645372390747]\n",
      "Val Batch Time: [0.09331703186035156]\n",
      "Val Batch: [2/5]\n",
      "Val Loss: [loss.val=1.2582557201385498] [loss.avg=0.8396615982055664]\n",
      "Val Batch Time: [0.07475964228312175]\n",
      "Val Batch: [3/5]\n",
      "Val Loss: [loss.val=0.9070227146148682] [loss.avg=0.8565018773078918]\n",
      "Val Batch Time: [0.065693199634552]\n",
      "Val Batch: [4/5]\n",
      "Val Loss: [loss.val=1.1161237955093384] [loss.avg=0.9084262609481811]\n",
      "Val Batch Time: [0.06011347770690918]\n",
      "New best model by [Val Loss = 0.9084262609481811]\n",
      "Saving model checkpoint\n",
      "Epoch: [9] | Train Batch: [0/5]\n",
      "Train Loss: [loss.val=0.680351734161377] [loss.avg=0.680351734161377]\n",
      "Train Batch Time: [0.23903727531433105]\n",
      "Epoch: [9] | Train Batch: [1/5]\n",
      "Train Loss: [loss.val=0.6571194529533386] [loss.avg=0.6687355935573578]\n",
      "Train Batch Time: [0.18920445442199707]\n",
      "Epoch: [9] | Train Batch: [2/5]\n",
      "Train Loss: [loss.val=0.6877673864364624] [loss.avg=0.6750795245170593]\n",
      "Train Batch Time: [0.17274880409240723]\n",
      "Epoch: [9] | Train Batch: [3/5]\n",
      "Train Loss: [loss.val=0.6817471981048584] [loss.avg=0.6767464429140091]\n",
      "Train Batch Time: [0.1645757555961609]\n",
      "Epoch: [9] | Train Batch: [4/5]\n",
      "Train Loss: [loss.val=0.6866355538368225] [loss.avg=0.6787242650985718]\n",
      "Train Batch Time: [0.1595439910888672]\n",
      "Val Batch: [0/5]\n",
      "Val Loss: [loss.val=0.8360713124275208] [loss.avg=0.8360713124275208]\n",
      "Val Batch Time: [0.15820932388305664]\n",
      "Val Batch: [1/5]\n",
      "Val Loss: [loss.val=0.8055257797241211] [loss.avg=0.8207985460758209]\n",
      "Val Batch Time: [0.09892678260803223]\n",
      "Val Batch: [2/5]\n",
      "Val Loss: [loss.val=0.6796351075172424] [loss.avg=0.7737440665562948]\n",
      "Val Batch Time: [0.0786728064219157]\n",
      "Val Batch: [3/5]\n",
      "Val Loss: [loss.val=0.6871961951255798] [loss.avg=0.752107098698616]\n",
      "Val Batch Time: [0.06840217113494873]\n",
      "Val Batch: [4/5]\n",
      "Val Loss: [loss.val=0.8890276551246643] [loss.avg=0.7794912099838257]\n",
      "Val Batch Time: [0.0623988151550293]\n",
      "New best model by [Val Loss = 0.7794912099838257]\n",
      "Saving model checkpoint\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data = {}\n",
    "plot_data['train_loss'] = zeros(EPOCHS)\n",
    "plot_data['val_loss'] = zeros(EPOCHS)\n",
    "plot_data['epoch'] = 0\n",
    "\n",
    "it_axes = arange(EPOCHS)\n",
    "_, ax1 = subplots()\n",
    "ax2 = ax1.twinx()\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Train Loss (r), Val Loss (y)')\n",
    "ax1.set_ylim([0, 2])\n",
    "best_loss = 1000\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    plot_data['epoch'] = epoch\n",
    "    # train for one epoch\n",
    "    plot_data = train(train_loader, model, criterion, optimizer, epoch, print_freq, plot_data, GPU)\n",
    "    # evaluate on validation set\n",
    "    plot_data = validate(val_loader, model, criterion, print_freq, plot_data, GPU)\n",
    "    \n",
    "    # remember best model and save checkpoint\n",
    "    is_best = plot_data['val_loss'][epoch] < best_loss\n",
    "    if is_best:\n",
    "        val_loss = plot_data['val_loss'][epoch]\n",
    "        logger.info(f'New best model by [Val Loss = {val_loss}]')\n",
    "        best_loss = plot_data['val_loss'][epoch]\n",
    "        filename = f'{DATASET_ROOT}/models/multi-modal-epoch-{epoch}'\n",
    "        save_checkpoint(model, filename)\n",
    "    \n",
    "    ax1.plot(it_axes[0: epoch+1], plot_data['train_loss'][0: epoch+1], 'r')\n",
    "    ax1.plot(it_axes[0: epoch+1], plot_data['val_loss'][0: epoch+1], 'y')\n",
    "    plt.grid(True)\n",
    "    plt.title('Multi-Modal CNN')\n",
    "    \n",
    "    # save plot to disk\n",
    "    title = f'{DATASET_ROOT}plots/epoch_{epoch}.jpg'\n",
    "    savefig(title, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e37f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
