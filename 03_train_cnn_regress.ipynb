{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60d1598f",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2856a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import zeros, arange, subplots, plt, savefig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc42e6e",
   "metadata": {},
   "source": [
    "#### Setup logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51a7f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "198f0e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using torch: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Using torch: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1164148",
   "metadata": {},
   "source": [
    "#### Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d2a5cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = './data/'\n",
    "CAPTIONS_TRAIN_SET_PATH = 'embedding/caption_embedding.csv'\n",
    "CAPTIONS_VALIDATION_SET_PATH = 'embedding/caption_embedding.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab170961",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSIONALITY = 1000 # number of CNN outputs = dimensionality of the captions word2vec model\n",
    "BATCH_SIZE = 4 \n",
    "EPOCHS = 10\n",
    "WORKERS = 4 # number of data loading workers = CPU cores\n",
    "GPU = 0\n",
    "LR = 0.01 \n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0f14d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e8df42",
   "metadata": {},
   "source": [
    "#### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d83a6bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, embedding_dimensionality):\n",
    "        super(Model, self).__init__()\n",
    "        self.cnn = models.resnet50(pretrained=True, num_classes=embedding_dimensionality)\n",
    "\n",
    "    def forward(self, image):\n",
    "        x = self.cnn(image)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e079d",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eb760d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss().cuda(GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e14abd5",
   "metadata": {},
   "source": [
    "Create ResNet50 model with custom number of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7379b79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(embedding_dimensionality=EMBEDDING_DIMENSIONALITY).cuda(GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98fdc2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.01\n",
       "    weight_decay: 0.0001\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2663547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.DataParallel(model, device_ids=[GPU])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83438b43",
   "metadata": {},
   "source": [
    "#### Define and load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d43e4bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, root_dir, split, embedding_dimensionality):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.embedding_dimensionality = embedding_dimensionality\n",
    "        self.preprocess = transforms.Compose([transforms.RandomHorizontalFlip(), \n",
    "                                              transforms.RandomCrop(224), \n",
    "                                              transforms.ToTensor(), \n",
    "                                              transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                                   std=[0.229, 0.224, 0.225])])\n",
    "        logger.info(f'Loading data from {split}')\n",
    "        \n",
    "        # count number of images in the split\n",
    "        n = 0\n",
    "        with open(f'{DATASET_ROOT}{CAPTIONS_TRAIN_SET_PATH}', 'r') as f:\n",
    "            for _, _ in enumerate(f):\n",
    "                n += 1\n",
    "                \n",
    "        # placeholder for image ids - dummy bytes\n",
    "        self.img_ids = np.empty([n], dtype='S50')\n",
    "        # placeholder for captions embedding - [number of captions * vector dimension]\n",
    "        self.captions_embeddings = np.zeros((n, self.embedding_dimensionality), dtype=np.float32)\n",
    "        \n",
    "        # populate the placeholders \n",
    "        with open(f'{DATASET_ROOT}{CAPTIONS_TRAIN_SET_PATH}', 'r') as f:\n",
    "            for idx, row in enumerate(f):\n",
    "                uid, vec = row.split('\\t')\n",
    "                vec = vec.strip().split(',')\n",
    "                self.img_ids[idx] = uid\n",
    "                for i in range(self.embedding_dimensionality):\n",
    "                    self.captions_embeddings[idx, i] = float(vec[i])\n",
    "        logger.info(f'Caption embedding shape = {self.captions_embeddings[0].shape}')\n",
    "        logger.info('Data loading done.')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx].decode('utf-8')\n",
    "        img = Image.open(f'{DATASET_ROOT}images/{img_id}.jpg').convert('RGB')\n",
    "        img_tensor = self.preprocess(img)\n",
    "        target_tensor = torch.from_numpy(self.captions_embeddings[idx, :])\n",
    "        return img_id, img_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12932d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data from embedding/caption_embedding.csv\n",
      "Caption embedding shape = (1000,)\n",
      "Data loading done.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(DATASET_ROOT, CAPTIONS_TRAIN_SET_PATH, EMBEDDING_DIMENSIONALITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b120758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b414c413",
   "metadata": {},
   "source": [
    "Dataset object wraps - image id, image tensor and the caption embedding tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce41e8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id, image_tensor, caption_embedding_tensor = train_dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2079b9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1489658491986857252'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f8f6903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b5572ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_embedding_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75f8a886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data from embedding/caption_embedding.csv\n",
      "Caption embedding shape = (1000,)\n",
      "Data loading done.\n"
     ]
    }
   ],
   "source": [
    "val_dataset = Dataset(DATASET_ROOT, CAPTIONS_VALIDATION_SET_PATH, EMBEDDING_DIMENSIONALITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a913a2e6",
   "metadata": {},
   "source": [
    "`pin_memory` allows better transferring of samples to GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1e69841",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           shuffle=True, \n",
    "                                           num_workers=WORKERS, \n",
    "                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0f21ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': <__main__.Dataset at 0x7f4dab0fc208>,\n",
       " 'num_workers': 4,\n",
       " 'pin_memory': True,\n",
       " 'timeout': 0,\n",
       " 'worker_init_fn': None,\n",
       " '_DataLoader__multiprocessing_context': None,\n",
       " '_dataset_kind': 0,\n",
       " 'batch_size': 4,\n",
       " 'drop_last': False,\n",
       " 'sampler': <torch.utils.data.sampler.RandomSampler at 0x7f4db01d1b00>,\n",
       " 'batch_sampler': <torch.utils.data.sampler.BatchSampler at 0x7f4db01d1b38>,\n",
       " 'collate_fn': <function torch.utils.data._utils.collate.default_collate(batch)>,\n",
       " '_DataLoader__initialized': True,\n",
       " '_IterableDataset_len_called': None}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52cda659",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(val_dataset, \n",
    "                                         batch_size=BATCH_SIZE, \n",
    "                                         shuffle=True, \n",
    "                                         num_workers=WORKERS, \n",
    "                                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc5e659",
   "metadata": {},
   "source": [
    "#### Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a93fc42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ba8a5c",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf5533cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_freq = 1 # how frequently to print loss value to screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9065487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, print_freq, plot_data, gpu):\n",
    "    batch_time = AverageMeter()\n",
    "    loss_meter = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    end = time.time()\n",
    "    for i, (img_id, image, target) in enumerate(train_loader):\n",
    "        target_var = torch.autograd.Variable(target).cuda(GPU)\n",
    "        image_var = torch.autograd.Variable(image)\n",
    "\n",
    "        # compute output\n",
    "        output = model(image_var)\n",
    "        loss = criterion(output, target_var)\n",
    "        \n",
    "        # measure and record loss\n",
    "        # loss.data.item() => loss value\n",
    "        # image.size()[0] => batch size\n",
    "        loss_meter.update(loss.data.item(), image.size()[0])\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        if i % print_freq == 0:\n",
    "            logger.info(f'Epoch: [{epoch}][{i}/{len(train_loader)}]')\n",
    "            logger.info(f'Train Loss: [loss.val={loss_meter.val}] [loss.avg={loss_meter.avg}]')\n",
    "            logger.info(f'Train Batch Time: [{batch_time.avg}]')\n",
    "            \n",
    "    plot_data['train_loss'][plot_data['epoch']] = loss_meter.avg\n",
    "    \n",
    "    return plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32b0086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, print_freq, plot_data, gpu):\n",
    "    with torch.no_grad():\n",
    "        batch_time = AverageMeter()\n",
    "        loss_meter = AverageMeter()\n",
    "\n",
    "        # switch to evaluate mode\n",
    "        model.eval()\n",
    "\n",
    "        end = time.time()\n",
    "        for i, (img_id, image, target) in enumerate(val_loader):\n",
    "            target_var = torch.autograd.Variable(target).cuda(GPU)\n",
    "            image_var = torch.autograd.Variable(image)\n",
    "\n",
    "            # compute output\n",
    "            output = model(image_var)\n",
    "            loss = criterion(output, target_var)\n",
    "\n",
    "            # measure and record loss\n",
    "            loss_meter.update(loss.data.item(), image.size()[0])\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            \n",
    "            if i % print_freq == 0:\n",
    "                logger.info(f'[{i}/{len(val_loader)}]')\n",
    "                logger.info(f'Val Loss: [loss.val={loss_meter.val}] [loss.avg={loss_meter.avg}]')\n",
    "                logger.info(f'Val Batch Time: [{batch_time.avg}]')\n",
    "        plot_data['val_loss'][plot_data['epoch']] = loss_meter.avg\n",
    "\n",
    "    return plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10c21efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, filename):\n",
    "    logger.info('Saving model checkpoint')\n",
    "    torch.save(model.state_dict(), filename + '.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "842b86b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/5]\n",
      "Train Loss: [loss.val=0.9510985612869263] [loss.avg=0.9510985612869263]\n",
      "Train Batch Time: [5.1960155963897705]\n",
      "Epoch: [0][1/5]\n",
      "Train Loss: [loss.val=2.4202065467834473] [loss.avg=1.6856525540351868]\n",
      "Train Batch Time: [2.663144588470459]\n",
      "Epoch: [0][2/5]\n",
      "Train Loss: [loss.val=1.177139163017273] [loss.avg=1.5161480903625488]\n",
      "Train Batch Time: [1.8219885031382244]\n",
      "Epoch: [0][3/5]\n",
      "Train Loss: [loss.val=0.9223955869674683] [loss.avg=1.3677099645137787]\n",
      "Train Batch Time: [1.4012518525123596]\n",
      "Epoch: [0][4/5]\n",
      "Train Loss: [loss.val=0.8869084715843201] [loss.avg=1.2715496659278869]\n",
      "Train Batch Time: [1.1489355087280273]\n",
      "[0/5]\n",
      "Val Loss: [loss.val=592204854722560.0] [loss.avg=592204854722560.0]\n",
      "Val Batch Time: [0.12799453735351562]\n",
      "[1/5]\n",
      "Val Loss: [loss.val=551293949050880.0] [loss.avg=571749401886720.0]\n",
      "Val Batch Time: [0.08309066295623779]\n",
      "[2/5]\n",
      "Val Loss: [loss.val=595681966292992.0] [loss.avg=579726923355477.4]\n",
      "Val Batch Time: [0.06808114051818848]\n",
      "[3/5]\n",
      "Val Loss: [loss.val=405343645466624.0] [loss.avg=536131103883264.0]\n",
      "Val Batch Time: [0.06044059991836548]\n",
      "[4/5]\n",
      "Val Loss: [loss.val=685232235741184.0] [loss.avg=565951330254848.0]\n",
      "Val Batch Time: [0.05581111907958984]\n",
      "Epoch: [1][0/5]\n",
      "Train Loss: [loss.val=1.0130144357681274] [loss.avg=1.0130144357681274]\n",
      "Train Batch Time: [0.2206113338470459]\n",
      "Epoch: [1][1/5]\n",
      "Train Loss: [loss.val=0.9481549263000488] [loss.avg=0.9805846810340881]\n",
      "Train Batch Time: [0.17987656593322754]\n",
      "Epoch: [1][2/5]\n",
      "Train Loss: [loss.val=0.8979036211967468] [loss.avg=0.9530243277549744]\n",
      "Train Batch Time: [0.16674272219340006]\n",
      "Epoch: [1][3/5]\n",
      "Train Loss: [loss.val=0.826628565788269] [loss.avg=0.921425387263298]\n",
      "Train Batch Time: [0.1601746678352356]\n",
      "Epoch: [1][4/5]\n",
      "Train Loss: [loss.val=0.81771320104599] [loss.avg=0.9006829500198364]\n",
      "Train Batch Time: [0.15593867301940917]\n",
      "[0/5]\n",
      "Val Loss: [loss.val=21505812480.0] [loss.avg=21505812480.0]\n",
      "Val Batch Time: [0.1341691017150879]\n",
      "[1/5]\n",
      "Val Loss: [loss.val=22978781184.0] [loss.avg=22242296832.0]\n",
      "Val Batch Time: [0.08636665344238281]\n",
      "[2/5]\n",
      "Val Loss: [loss.val=17618939904.0] [loss.avg=20701177856.0]\n",
      "Val Batch Time: [0.06998411814371745]\n",
      "[3/5]\n",
      "Val Loss: [loss.val=16013228032.0] [loss.avg=19529190400.0]\n",
      "Val Batch Time: [0.061743319034576416]\n",
      "[4/5]\n",
      "Val Loss: [loss.val=16873841664.0] [loss.avg=18998120652.8]\n",
      "Val Batch Time: [0.05693130493164063]\n",
      "Epoch: [2][0/5]\n",
      "Train Loss: [loss.val=0.7768837213516235] [loss.avg=0.7768837213516235]\n",
      "Train Batch Time: [0.219862699508667]\n",
      "Epoch: [2][1/5]\n",
      "Train Loss: [loss.val=0.7283805012702942] [loss.avg=0.7526321113109589]\n",
      "Train Batch Time: [0.17962861061096191]\n",
      "Epoch: [2][2/5]\n",
      "Train Loss: [loss.val=0.7628538608551025] [loss.avg=0.7560393611590067]\n",
      "Train Batch Time: [0.16643134752909342]\n",
      "Epoch: [2][3/5]\n",
      "Train Loss: [loss.val=0.726635754108429] [loss.avg=0.7486884593963623]\n",
      "Train Batch Time: [0.15977764129638672]\n",
      "Epoch: [2][4/5]\n",
      "Train Loss: [loss.val=0.7189408540725708] [loss.avg=0.742738938331604]\n",
      "Train Batch Time: [0.155861759185791]\n",
      "[0/5]\n",
      "Val Loss: [loss.val=299976.15625] [loss.avg=299976.15625]\n",
      "Val Batch Time: [0.1388230323791504]\n",
      "[1/5]\n",
      "Val Loss: [loss.val=218756.125] [loss.avg=259366.140625]\n",
      "Val Batch Time: [0.08883869647979736]\n",
      "[2/5]\n",
      "Val Loss: [loss.val=283254.0] [loss.avg=267328.7604166667]\n",
      "Val Batch Time: [0.07187199592590332]\n",
      "[3/5]\n",
      "Val Loss: [loss.val=228421.71875] [loss.avg=257602.0]\n",
      "Val Batch Time: [0.0632132887840271]\n",
      "[4/5]\n",
      "Val Loss: [loss.val=289151.125] [loss.avg=263911.825]\n",
      "Val Batch Time: [0.05801482200622558]\n",
      "Epoch: [3][0/5]\n",
      "Train Loss: [loss.val=0.7191405892372131] [loss.avg=0.7191405892372131]\n",
      "Train Batch Time: [0.23669815063476562]\n",
      "Epoch: [3][1/5]\n",
      "Train Loss: [loss.val=0.7049260139465332] [loss.avg=0.7120333015918732]\n",
      "Train Batch Time: [0.1883763074874878]\n",
      "Epoch: [3][2/5]\n",
      "Train Loss: [loss.val=0.7101747393608093] [loss.avg=0.7114137808481852]\n",
      "Train Batch Time: [0.17256943384806314]\n",
      "Epoch: [3][3/5]\n",
      "Train Loss: [loss.val=0.6958812475204468] [loss.avg=0.7075306475162506]\n",
      "Train Batch Time: [0.16430598497390747]\n",
      "Epoch: [3][4/5]\n",
      "Train Loss: [loss.val=0.7102503180503845] [loss.avg=0.7080745816230773]\n",
      "Train Batch Time: [0.15940065383911134]\n",
      "[0/5]\n",
      "Val Loss: [loss.val=1691.14208984375] [loss.avg=1691.14208984375]\n",
      "Val Batch Time: [0.1375107765197754]\n",
      "[1/5]\n",
      "Val Loss: [loss.val=1306.9140625] [loss.avg=1499.028076171875]\n",
      "Val Batch Time: [0.0886009931564331]\n",
      "[2/5]\n",
      "Val Loss: [loss.val=1854.9046630859375] [loss.avg=1617.6536051432292]\n",
      "Val Batch Time: [0.07171312967936198]\n",
      "[3/5]\n",
      "Val Loss: [loss.val=1868.5450439453125] [loss.avg=1680.37646484375]\n",
      "Val Batch Time: [0.06317228078842163]\n",
      "[4/5]\n",
      "Val Loss: [loss.val=1721.1773681640625] [loss.avg=1688.5366455078124]\n",
      "Val Batch Time: [0.057924079895019534]\n",
      "Epoch: [4][0/5]\n",
      "Train Loss: [loss.val=0.6992158889770508] [loss.avg=0.6992158889770508]\n",
      "Train Batch Time: [0.22369170188903809]\n",
      "Epoch: [4][1/5]\n",
      "Train Loss: [loss.val=0.7018366456031799] [loss.avg=0.7005262672901154]\n",
      "Train Batch Time: [0.18158257007598877]\n",
      "Epoch: [4][2/5]\n",
      "Train Loss: [loss.val=0.6937674880027771] [loss.avg=0.6982733408610026]\n",
      "Train Batch Time: [0.16764394442240396]\n",
      "Epoch: [4][3/5]\n",
      "Train Loss: [loss.val=0.6934311389923096] [loss.avg=0.6970627903938293]\n",
      "Train Batch Time: [0.16056478023529053]\n",
      "Epoch: [4][4/5]\n",
      "Train Loss: [loss.val=0.6975440979003906] [loss.avg=0.6971590518951416]\n",
      "Train Batch Time: [0.15639567375183105]\n",
      "[0/5]\n",
      "Val Loss: [loss.val=137.29994201660156] [loss.avg=137.29994201660156]\n",
      "Val Batch Time: [0.1399068832397461]\n",
      "[1/5]\n",
      "Val Loss: [loss.val=137.7161865234375] [loss.avg=137.50806427001953]\n",
      "Val Batch Time: [0.08970308303833008]\n",
      "[2/5]\n",
      "Val Loss: [loss.val=178.8182830810547] [loss.avg=151.27813720703125]\n",
      "Val Batch Time: [0.07235185305277507]\n",
      "[3/5]\n",
      "Val Loss: [loss.val=128.0736541748047] [loss.avg=145.4770164489746]\n",
      "Val Batch Time: [0.06367027759552002]\n",
      "[4/5]\n",
      "Val Loss: [loss.val=143.283203125] [loss.avg=145.0382537841797]\n",
      "Val Batch Time: [0.058499813079833984]\n",
      "New best model by [Val Loss = 145.0382537841797]\n",
      "Saving model checkpoint\n",
      "Epoch: [5][0/5]\n",
      "Train Loss: [loss.val=0.6915939450263977] [loss.avg=0.6915939450263977]\n",
      "Train Batch Time: [0.23428702354431152]\n",
      "Epoch: [5][1/5]\n",
      "Train Loss: [loss.val=0.6917527318000793] [loss.avg=0.6916733384132385]\n",
      "Train Batch Time: [0.1869204044342041]\n",
      "Epoch: [5][2/5]\n",
      "Train Loss: [loss.val=0.6846379041671753] [loss.avg=0.6893281936645508]\n",
      "Train Batch Time: [0.1712327003479004]\n",
      "Epoch: [5][3/5]\n",
      "Train Loss: [loss.val=0.6929170489311218] [loss.avg=0.6902254074811935]\n",
      "Train Batch Time: [0.16336584091186523]\n",
      "Epoch: [5][4/5]\n",
      "Train Loss: [loss.val=0.6923992037773132] [loss.avg=0.6906601667404175]\n",
      "Train Batch Time: [0.1586167812347412]\n",
      "[0/5]\n",
      "Val Loss: [loss.val=9.642428398132324] [loss.avg=9.642428398132324]\n",
      "Val Batch Time: [0.13114595413208008]\n",
      "[1/5]\n",
      "Val Loss: [loss.val=24.574216842651367] [loss.avg=17.108322620391846]\n",
      "Val Batch Time: [0.08491086959838867]\n",
      "[2/5]\n",
      "Val Loss: [loss.val=11.375105857849121] [loss.avg=15.197250366210938]\n",
      "Val Batch Time: [0.06914615631103516]\n",
      "[3/5]\n",
      "Val Loss: [loss.val=19.827899932861328] [loss.avg=16.354912757873535]\n",
      "Val Batch Time: [0.06124991178512573]\n",
      "[4/5]\n",
      "Val Loss: [loss.val=20.958803176879883] [loss.avg=17.275690841674805]\n",
      "Val Batch Time: [0.05642313957214355]\n",
      "New best model by [Val Loss = 17.275690841674805]\n",
      "Saving model checkpoint\n",
      "Epoch: [6][0/5]\n",
      "Train Loss: [loss.val=0.6928911209106445] [loss.avg=0.6928911209106445]\n",
      "Train Batch Time: [0.21997356414794922]\n",
      "Epoch: [6][1/5]\n",
      "Train Loss: [loss.val=0.6819558143615723] [loss.avg=0.6874234676361084]\n",
      "Train Batch Time: [0.17985379695892334]\n",
      "Epoch: [6][2/5]\n",
      "Train Loss: [loss.val=0.6950990557670593] [loss.avg=0.689981997013092]\n",
      "Train Batch Time: [0.16634257634480795]\n",
      "Epoch: [6][3/5]\n",
      "Train Loss: [loss.val=0.69085693359375] [loss.avg=0.6902007311582565]\n",
      "Train Batch Time: [0.15976667404174805]\n",
      "Epoch: [6][4/5]\n",
      "Train Loss: [loss.val=0.6919114589691162] [loss.avg=0.6905428767204285]\n",
      "Train Batch Time: [0.15574126243591307]\n",
      "[0/5]\n",
      "Val Loss: [loss.val=1.8673597574234009] [loss.avg=1.8673597574234009]\n",
      "Val Batch Time: [0.1354377269744873]\n",
      "[1/5]\n",
      "Val Loss: [loss.val=1.4027743339538574] [loss.avg=1.6350670456886292]\n",
      "Val Batch Time: [0.08713984489440918]\n",
      "[2/5]\n",
      "Val Loss: [loss.val=2.2869415283203125] [loss.avg=1.8523585398991902]\n",
      "Val Batch Time: [0.07066734631856282]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3/5]\n",
      "Val Loss: [loss.val=2.8184947967529297] [loss.avg=2.093892604112625]\n",
      "Val Batch Time: [0.06240361928939819]\n",
      "[4/5]\n",
      "Val Loss: [loss.val=2.723625898361206] [loss.avg=2.219839262962341]\n",
      "Val Batch Time: [0.05742311477661133]\n",
      "New best model by [Val Loss = 2.219839262962341]\n",
      "Saving model checkpoint\n",
      "Epoch: [7][0/5]\n",
      "Train Loss: [loss.val=0.6921711564064026] [loss.avg=0.6921711564064026]\n",
      "Train Batch Time: [0.22690725326538086]\n",
      "Epoch: [7][1/5]\n",
      "Train Loss: [loss.val=0.6652157306671143] [loss.avg=0.6786934435367584]\n",
      "Train Batch Time: [0.18319463729858398]\n",
      "Epoch: [7][2/5]\n",
      "Train Loss: [loss.val=0.6822831034660339] [loss.avg=0.679889996846517]\n",
      "Train Batch Time: [0.16869584719340006]\n",
      "Epoch: [7][3/5]\n",
      "Train Loss: [loss.val=0.6938483119010925] [loss.avg=0.6833795756101608]\n",
      "Train Batch Time: [0.16140437126159668]\n",
      "Epoch: [7][4/5]\n",
      "Train Loss: [loss.val=0.6885959506034851] [loss.avg=0.6844228506088257]\n",
      "Train Batch Time: [0.1570584297180176]\n",
      "[0/5]\n",
      "Val Loss: [loss.val=0.6876615285873413] [loss.avg=0.6876615285873413]\n",
      "Val Batch Time: [0.13510966300964355]\n",
      "[1/5]\n",
      "Val Loss: [loss.val=0.6866534948348999] [loss.avg=0.6871575117111206]\n",
      "Val Batch Time: [0.08699512481689453]\n",
      "[2/5]\n",
      "Val Loss: [loss.val=0.6685866117477417] [loss.avg=0.6809672117233276]\n",
      "Val Batch Time: [0.07035064697265625]\n",
      "[3/5]\n",
      "Val Loss: [loss.val=0.6863694190979004] [loss.avg=0.6823177635669708]\n",
      "Val Batch Time: [0.06197047233581543]\n",
      "[4/5]\n",
      "Val Loss: [loss.val=1.369689702987671] [loss.avg=0.8197921514511108]\n",
      "Val Batch Time: [0.05712251663208008]\n",
      "New best model by [Val Loss = 0.8197921514511108]\n",
      "Saving model checkpoint\n",
      "Epoch: [8][0/5]\n",
      "Train Loss: [loss.val=0.6899225115776062] [loss.avg=0.6899225115776062]\n",
      "Train Batch Time: [0.2258138656616211]\n",
      "Epoch: [8][1/5]\n",
      "Train Loss: [loss.val=0.6914985775947571] [loss.avg=0.6907105445861816]\n",
      "Train Batch Time: [0.18263041973114014]\n",
      "Epoch: [8][2/5]\n",
      "Train Loss: [loss.val=0.6806822419166565] [loss.avg=0.6873677770296732]\n",
      "Train Batch Time: [0.16832184791564941]\n",
      "Epoch: [8][3/5]\n",
      "Train Loss: [loss.val=0.6897014379501343] [loss.avg=0.6879511922597885]\n",
      "Train Batch Time: [0.16118335723876953]\n",
      "Epoch: [8][4/5]\n",
      "Train Loss: [loss.val=0.6846224069595337] [loss.avg=0.6872854351997375]\n",
      "Train Batch Time: [0.15685815811157228]\n",
      "[0/5]\n",
      "Val Loss: [loss.val=0.6871076822280884] [loss.avg=0.6871076822280884]\n",
      "Val Batch Time: [0.13782000541687012]\n",
      "[1/5]\n",
      "Val Loss: [loss.val=1.420502781867981] [loss.avg=1.0538052320480347]\n",
      "Val Batch Time: [0.08821988105773926]\n",
      "[2/5]\n",
      "Val Loss: [loss.val=0.6969107389450073] [loss.avg=0.9348404010136923]\n",
      "Val Batch Time: [0.07126132647196452]\n",
      "[3/5]\n",
      "Val Loss: [loss.val=0.7062835097312927] [loss.avg=0.8777011781930923]\n",
      "Val Batch Time: [0.06272178888320923]\n",
      "[4/5]\n",
      "Val Loss: [loss.val=0.6891437768936157] [loss.avg=0.839989697933197]\n",
      "Val Batch Time: [0.05790553092956543]\n",
      "Epoch: [9][0/5]\n",
      "Train Loss: [loss.val=0.6805098056793213] [loss.avg=0.6805098056793213]\n",
      "Train Batch Time: [0.23093605041503906]\n",
      "Epoch: [9][1/5]\n",
      "Train Loss: [loss.val=0.6812300086021423] [loss.avg=0.6808699071407318]\n",
      "Train Batch Time: [0.18533563613891602]\n",
      "Epoch: [9][2/5]\n",
      "Train Loss: [loss.val=0.6832241415977478] [loss.avg=0.6816546519597372]\n",
      "Train Batch Time: [0.1701502005259196]\n",
      "Epoch: [9][3/5]\n",
      "Train Loss: [loss.val=0.6873272061347961] [loss.avg=0.6830727905035019]\n",
      "Train Batch Time: [0.16255342960357666]\n",
      "Epoch: [9][4/5]\n",
      "Train Loss: [loss.val=0.6877073049545288] [loss.avg=0.6839996933937073]\n",
      "Train Batch Time: [0.157959508895874]\n",
      "[0/5]\n",
      "Val Loss: [loss.val=0.6826240420341492] [loss.avg=0.6826240420341492]\n",
      "Val Batch Time: [0.14231610298156738]\n",
      "[1/5]\n",
      "Val Loss: [loss.val=1.0332162380218506] [loss.avg=0.8579201400279999]\n",
      "Val Batch Time: [0.09070479869842529]\n",
      "[2/5]\n",
      "Val Loss: [loss.val=0.6827878952026367] [loss.avg=0.7995427250862122]\n",
      "Val Batch Time: [0.07282217343648274]\n",
      "[3/5]\n",
      "Val Loss: [loss.val=0.6917301416397095] [loss.avg=0.7725895792245865]\n",
      "Val Batch Time: [0.06401169300079346]\n",
      "[4/5]\n",
      "Val Loss: [loss.val=0.7119988203048706] [loss.avg=0.7604714274406433]\n",
      "Val Batch Time: [0.058616256713867186]\n",
      "New best model by [Val Loss = 0.7604714274406433]\n",
      "Saving model checkpoint\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data = {}\n",
    "plot_data['train_loss'] = zeros(EPOCHS)\n",
    "plot_data['val_loss'] = zeros(EPOCHS)\n",
    "plot_data['epoch'] = 0\n",
    "\n",
    "it_axes = arange(EPOCHS)\n",
    "_, ax1 = subplots()\n",
    "ax2 = ax1.twinx()\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Train Loss (r), Val Loss (y)')\n",
    "ax1.set_ylim([0, 2])\n",
    "best_loss = 1000\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    plot_data['epoch'] = epoch\n",
    "    # train for one epoch\n",
    "    plot_data = train(train_loader, model, criterion, optimizer, epoch, print_freq, plot_data, GPU)\n",
    "    # evaluate on validation set\n",
    "    plot_data = validate(val_loader, model, criterion, print_freq, plot_data, GPU)\n",
    "    \n",
    "    # remember best model and save checkpoint\n",
    "    is_best = plot_data['val_loss'][epoch] < best_loss\n",
    "    if is_best:\n",
    "        val_loss = plot_data['val_loss'][epoch]\n",
    "        logger.info(f'New best model by [Val Loss = {val_loss}]')\n",
    "        best_loss = plot_data['val_loss'][epoch]\n",
    "        filename = f'{DATASET_ROOT}/models/multi-modal-epoch-{epoch}'\n",
    "        save_checkpoint(model, filename)\n",
    "    \n",
    "    ax1.plot(it_axes[0: epoch+1], plot_data['train_loss'][0: epoch+1], 'r')\n",
    "    ax1.plot(it_axes[0: epoch+1], plot_data['val_loss'][0: epoch+1], 'y')\n",
    "    plt.grid(True)\n",
    "    plt.title('Multi-Modal CNN')\n",
    "    \n",
    "    # save plot to disk\n",
    "    title = f'{DATASET_ROOT}plots/epoch_{epoch}.jpg'\n",
    "    savefig(title, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b908965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
